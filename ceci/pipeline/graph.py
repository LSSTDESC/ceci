import networkx
import collections

def build_graph(stage_names, stage_classes, stage_aliases, overall_inputs):
    """
    Build a directed graph of the pipeline stages and their inputs and outputs.

    This also checks for various pipeline problems like missing inputs,
    multiple stages generating the same output, and cyclic dependencies.
    
    Parameters
    ----------
    stage_names : `list` of `str`
        The names of the stages in the pipeline
    stage_classes : `list` of `PipelineStage` subclasses
        The classes of the stages in the pipeline
    stage_aliases : `list` of `dict`
        A list of dictionaries mapping tags to aliases for each stage
    overall_inputs : `dict` of `str: str`
        A list of overall input files that are not generated by any stage but are needed by the pipeline

    Returns
    -------
    graph : `networkx.DiGraph`
        A directed graph representing the pipeline stages, their inputs, and outputs
    """

    # Build a graph with all the stages and files in it
    graph = networkx.DiGraph()

    # First add all the overall inputs
    for inp in overall_inputs:
        graph.add_node(inp, type="input")
    
    # Now go through all the stages. We do this in two passes,
    # one to add the stages themselves and their outputs, and one
    # to connect in all their inputs
    for stage_name, stage_class, aliases in zip(stage_names, stage_classes, stage_aliases):
        if stage_name in graph.nodes:
            raise ValueError(f"Stage {stage_name} is used multiple times in the pipeline")
        graph.add_node(stage_name, type="stage", stage_class=stage_class)

        # Now set up all the outputs of the stage, first checking to see if it has been aliased
        for tag1 in stage_class.output_tags():
            tag = aliases.get(tag1, tag1)
            if tag in overall_inputs:
                raise ValueError(f"Tag {tag} is generated by pipeline stage {stage_name} but is also listed as an overall input to the pipeline")
            graph.add_node(tag, type="output", creator=stage_class, original_tag=tag1)
            graph.add_edge(stage_name, tag)

    # Now check for outputs that are generated by multiple stages
    for node in graph.nodes:
        if graph.nodes[node]["type"] == "output":
            parents = list(graph.predecessors(node))
            if len(parents) > 1:
                raise ValueError(f"Output {node} is generated by multiple stages: {parents}")
            

    # Now go through and add in all the inputs needed for each stage:
    missing = collections.defaultdict(list)
    for stage_name, stage_class, aliases in zip(stage_names, stage_classes, stage_aliases):
        for tag in stage_class.input_tags():
            tag = aliases.get(tag, tag)
            if tag not in graph.nodes:
                missing[tag].append(stage_name)
            else:
                graph.add_edge(tag, stage_name)
    
    if missing:
        msg = []
        for tag, stages in missing.items():
            stages = ", ".join(stages)
            msg.append(f"- {tag} - needed by stages {stages}")
        msg = "\n.join(msg)"
        raise ValueError(f"The following files are needed by the pipeline but not generated by any stage:\n{msg}")

    # Now check for cycles in the graph
    cycles = list(networkx.simple_cycles(graph))
    if cycles:
        msg = []
        for cycle in cycles:
            cycle_text = " -> ".join(cycle)
            cycle_text += " -> " + cycle[0]
            msg.append(f"- {cycle_text}")
        raise ValueError(f"Your pipeline is cyclic. The following cycles were found:\n{msg}")
    
    return graph

def get_static_ordering(graph):
    # get a topological ordering of the graph
    # this will be a list of all the nodes in the graph, including the input and output files
    ordering = list(networkx.topological_sort(graph))

    # cut down to just the stages
    ordering = [node for node in ordering if graph.nodes[node]["type"] == "stage"]
   
    return ordering

def trim_pipeline_graph(graph, from_=None, to_=None):
    """
    Trim the pipeline graph to only include the stages between `from_` and `to_`.

    Either `from_` or `to_` can be `None`, in which case no trimming is done in that direction.

    They can be single strings, or lists of strings, in which case the graph is trimmed to include all stages
    between any of the stages in `from_` and any of the stages in `to

    Parameters
    ----------
    graph : `networkx.DiGraph`
        The original pipeline graph
    from_ : `str`
        The name of the stage(s) or file(s) to start from.
    to_ : `str`
        The name of the stage(s) or file(s) to to end at.

    Returns
    -------
    trimmed_graph : `networkx.DiGraph`
        The trimmed pipeline graph
    """
    if isinstance(from_, str):
        from_ = [from_]
    if isinstance(to_, str):
        to_ = [to_]

    # Descendents is all nodes that are descendend from any of the nodes in `from_`
    # if from_ is None then it includes all nodes in the graph
    descendents = set()

    if from_ is None:
        descendents.update(graph.nodes)
    else:
        for node in from_:
            if node not in graph.nodes:
                raise ValueError(f"Node {node} is not in the pipeline graph")
            # first include the nodes in `from_` themselves
            descendents.add(node)
            descendents.update(networkx.descendants(graph, node))

    # Ancestors is all nodes that are ancestors of any of the nodes in `to_`
    # if to_ is None then it includes all nodes in the graph
    ancestors = set()
    if to_ is None:
        ancestors.update(graph.nodes)
    else:
        for node in to_:
            if node not in graph.nodes:
                raise ValueError(f"Node {node} is not in the pipeline graph")
            ancestors.add(node)
            ancestors.update(networkx.ancestors(graph, node))

    # Now we want to keep everything that is both a descendent of `from_` and an ancestor of `to_`
    # This is the intersection of the two sets
    nodes_to_keep = descendents.intersection(ancestors)

    # We also want to keep the input and output files for the stages we are keeping
    # (though not the stages that make those)
    more_to_keep = set()
    for node in nodes_to_keep:
        if graph.nodes[node]["type"] == "stage":
            # Get all the outputs of this stage
            outputs = [n for n in graph.successors(node) if graph.nodes[n]["type"] == "output"]
            more_to_keep.update(outputs)
            # same for the inputs to all the stages
            inputs = [n for n in graph.predecessors(node) if graph.nodes[n]["type"] in ["input", "output"]]
            more_to_keep.update(inputs)
    nodes_to_keep.update(more_to_keep)

    # Create a subgraph with just these nodes
    trimmed_graph = graph.subgraph(nodes_to_keep).copy()

    # Some stages that were previously outputs of existing stages
    # must now be overall inputs to the pipeline instead because
    # the stage that generated them has been trimmed. We need to keep
    # track of this
    converted_inputs = {}
    for node_name in list(trimmed_graph.nodes):
        node = trimmed_graph.nodes[node_name]

        if node["type"] == "output":
            # The node will be listed as an output but will have no
            # predecessor creating it now.
            predecessors = list(trimmed_graph.predecessors(node_name))
            if not predecessors:
                creator_stage_class = node["creator"]
                original_tag = node["original_tag"]
                # use the original tag to find the file type object
                for t, typ in creator_stage_class.outputs:
                    if t == original_tag:
                        ftype = typ
                        break
                else:
                    raise RuntimeError("This should never happen - please open an issue")
                # the node_name is the aliased_tag, so this file name should be correct
                file_name = ftype.make_name(node_name)
                converted_inputs[node_name] = file_name

    return trimmed_graph, converted_inputs
